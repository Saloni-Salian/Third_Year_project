{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: install in /usr/local/lib/python3.10/site-packages (1.3.5)\n",
      "Requirement already satisfied: optimum[onnxruntime] in /usr/local/lib/python3.10/site-packages (1.17.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/site-packages (from optimum[onnxruntime]) (15.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from optimum[onnxruntime]) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/salonisalian/Library/Python/3.10/lib/python/site-packages (from optimum[onnxruntime]) (23.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from optimum[onnxruntime]) (1.12)\n",
      "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/site-packages (from optimum[onnxruntime]) (2.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.10/site-packages (from optimum[onnxruntime]) (0.20.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/site-packages (from optimum[onnxruntime]) (2.17.1)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.10/site-packages (from optimum[onnxruntime]) (4.37.2)\n",
      "Requirement already satisfied: onnxruntime>=1.11.0 in /usr/local/lib/python3.10/site-packages (from optimum[onnxruntime]) (1.17.0)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/site-packages (from optimum[onnxruntime]) (0.4.1)\n",
      "Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.10/site-packages (from optimum[onnxruntime]) (4.25.3)\n",
      "Requirement already satisfied: onnx in /usr/local/lib/python3.10/site-packages (from optimum[onnxruntime]) (1.15.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets->optimum[onnxruntime]) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from datasets->optimum[onnxruntime]) (2.31.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets->optimum[onnxruntime]) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/site-packages (from datasets->optimum[onnxruntime]) (4.66.2)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from datasets->optimum[onnxruntime]) (2023.10.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/site-packages (from datasets->optimum[onnxruntime]) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from datasets->optimum[onnxruntime]) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from datasets->optimum[onnxruntime]) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from datasets->optimum[onnxruntime]) (6.0.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/site-packages (from datasets->optimum[onnxruntime]) (3.4.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/site-packages (from datasets->optimum[onnxruntime]) (15.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/site-packages (from datasets->optimum[onnxruntime]) (0.70.16)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime]) (4.9.0)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/site-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (23.5.26)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=1.11->optimum[onnxruntime]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (0.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (2023.12.25)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum[onnxruntime]) (0.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/site-packages (from coloredlogs->optimum[onnxruntime]) (10.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/site-packages (from evaluate->optimum[onnxruntime]) (0.18.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/site-packages (from sympy->optimum[onnxruntime]) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->datasets->optimum[onnxruntime]) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets->optimum[onnxruntime]) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets->optimum[onnxruntime]) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets->optimum[onnxruntime]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->datasets->optimum[onnxruntime]) (2.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/salonisalian/Library/Python/3.10/lib/python/site-packages (from pandas->datasets->optimum[onnxruntime]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets->optimum[onnxruntime]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets->optimum[onnxruntime]) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/salonisalian/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum[onnxruntime]) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade-strategy eager install optimum[onnxruntime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git@main\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision main) to /private/var/folders/hr/pk_thz1s2w1b7rh7_xc8vxrh0000gn/T/pip-req-build-fy2vjzxe\n",
      "  Running command git clone -q https://github.com/huggingface/transformers.git /private/var/folders/hr/pk_thz1s2w1b7rh7_xc8vxrh0000gn/T/pip-req-build-fy2vjzxe\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Applications/anaconda3/lib/python3.8/site-packages (from transformers==4.39.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/lib/python3.8/site-packages (from transformers==4.39.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Applications/anaconda3/lib/python3.8/site-packages (from transformers==4.39.0.dev0) (1.21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Applications/anaconda3/lib/python3.8/site-packages (from transformers==4.39.0.dev0) (0.20.2)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/lib/python3.8/site-packages (from transformers==4.39.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Applications/anaconda3/lib/python3.8/site-packages (from transformers==4.39.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Applications/anaconda3/lib/python3.8/site-packages (from transformers==4.39.0.dev0) (5.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/lib/python3.8/site-packages (from transformers==4.39.0.dev0) (20.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Applications/anaconda3/lib/python3.8/site-packages (from transformers==4.39.0.dev0) (0.15.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Applications/anaconda3/lib/python3.8/site-packages (from transformers==4.39.0.dev0) (2020.6.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.39.0.dev0) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.39.0.dev0) (2.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/lib/python3.8/site-packages (from requests->transformers==4.39.0.dev0) (3.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Applications/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Applications/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Applications/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.39.0.dev0) (2.4.7)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8551656 sha256=857edd367d7ed88b6cee6c328476bbca4bc6ab98542fc5af64dc107706a47624\n",
      "  Stored in directory: /private/var/folders/hr/pk_thz1s2w1b7rh7_xc8vxrh0000gn/T/pip-ephem-wheel-cache-eg8dyil6/wheels/0c/f1/cf/0c84f8631406672e9adab41401961ab0d771b0b9c6f7195624\n",
      "Successfully built transformers\n",
      "\u001b[31mERROR: spacy-transformers 1.1.9 has requirement transformers<4.26.0,>=3.4.0, but you'll have transformers 4.39.0.dev0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: fastt5 0.1.4 has requirement onnxruntime==1.10.0, but you'll have onnxruntime 1.16.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.37.2\n",
      "    Uninstalling transformers-4.37.2:\n",
      "      Successfully uninstalled transformers-4.37.2\n",
      "Successfully installed transformers-4.39.0.dev0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/huggingface/transformers.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = ORTModelForTokenClassification.from_pretrained(\"/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/model-quantized\")\n",
    "tokenizerdisq = AutoTokenizer.from_pretrained(\"/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/model-quantized\")\n",
    "onnx_ner = pipeline(\"token-classification\", model=quantized_model, tokenizer=tokenizerdisq, aggregation_strategy=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitise_data(text: str, entities: list) -> dict:\n",
    "    return_entities=[]\n",
    "    for entity in entities:\n",
    "        santitise_entity={}\n",
    "        santitise_entity['start'] = entity['start']\n",
    "        santitise_entity['end'] = entity['end']\n",
    "        santitise_entity['label'] = entity['entity_group']\n",
    "        return_entities.append(santitise_entity)\n",
    "    return {'text':text, 'ents':return_entities, 'title': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='In the bustling city of New York, nestled amidst the towering skyscrapers, lived two childhood friends, Emma and Jack. \\\n",
    "    They grew up in the vibrant neighborhood of Brooklyn, their bond forged through countless adventures and shared dreams. \\\n",
    "    Despite their different backgrounds, they were inseparable, their friendship standing the test of time. As they entered adulthood,\\\n",
    "    Emma pursued her passion for art, while Jack found success in the world of finance. Despite their divergent paths, they remained close, \\\n",
    "    supporting each other through lifes ups and downs. However, their lives took an unexpected turn when the art gallery, \"Canvas Creations,\" \\\n",
    "    faced financial difficulties. Struggling to keep her business afloat, Emma found herself on the brink of despair. Determined to help his \\\n",
    "    friend, Jack proposed a daring plan. He was a member of a secret organization known as \"The Brotherhood,\" a clandestine group dedicated to \\\n",
    "    aiding those in need. With their vast resources and network of contacts, The Brotherhood had the power to solve Emmas problems discreetly. \\\n",
    "    Despite Emmas initial reluctance, she eventually agreed, desperate to save her gallery from ruin. Under the cover of darkness, Jack \\\n",
    "    arranged a clandestine meeting with The Brotherhoods enigmatic leader, known only as \"The Guardian.\" In a dimly lit alleyway, Jack \\\n",
    "    explained Emmas predicament, pleading for The Brotherhoods assistance. Impressed by Jacks loyalty and determination, The Guardian agreed \\\n",
    "    to help, on one condition: Emma and Jack would owe The Brotherhood a favor, to be redeemed at a later date. With The Brotherhoods support,\\\n",
    "    Emmas gallery was saved from bankruptcy. Grateful for their assistance, Emma and Jack resumed their lives, hoping to put the ordeal \\\n",
    "    behind them. However, their newfound peace was short-lived when The Brotherhood called in their favor. Emma and Jack found themselves \\\n",
    "    embroiled in a dangerous mission, tasked with recovering a valuable artifact stolen from The Brotherhoods archives. With their lives on \\\n",
    "    the line, they embarked on a perilous journey, spanning continents and facing formidable adversaries. Along the way, they uncovered \\\n",
    "    secrets that shook the foundations of The Brotherhood, challenging their loyalty and trust. Despite the odds stacked against them, \\\n",
    "    Emma and Jack persevered, their friendship and determination guiding them through adversity. With their unwavering resolve, they \\\n",
    "    succeeded in retrieving the stolen artifact, earning The Brotherhoods gratitude and respect. However, their victory came at a cost:\\\n",
    "    they were forever bound to The Brotherhood, their lives intertwined with the clandestine organizations machinations. As they returned \\\n",
    "    to New York, Emma and Jack reflected on their harrowing ordeal, knowing that their lives would never be the same. Despite the dangers\\\n",
    "     that lay ahead, they faced the future together, united by their friendship and the bonds they had forged. For in the shadows of the \\\n",
    "    city that never sleeps, secrets lurked, and alliances shifted – but Emma and Jack knew that as long as they had each other, they could \\\n",
    "    overcome any challenge that came their way.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time to return results for 504 words of text is: 0.80\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time=time.time()\n",
    "pred = onnx_ner(text)\n",
    "pred = sanitise_data(text, pred)\n",
    "end_time=time.time()\n",
    "print (f\"The time to return results for 504 words of text is: {(end_time-start_time):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id=\"/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:/ner/model\"\n",
    "model_id='/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:/ner/model/model-quantized'\n",
    "tokenizer_id='/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "model = ORTModelForTokenClassification.from_pretrained(model_id)\n",
    "# model = ORTModelForTokenClassification.from_pretrained(\"jammmmmm/pii\", export=True)\n",
    "tokenizer.model_input_names = ['input_ids', 'attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found too many ONNX model files in /Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:/ner/model. ORTQuantizer does not support multi-file quantization. Please create separate ORTQuantizer instances for each model/file, by passing the argument `file_name` to ORTQuantizer.from_pretrained().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a4eed24611d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# create ORTQuantizer and define quantization configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mquantizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mORTQuantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mqconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoQuantizationConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavx512_vnni\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_static\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_channel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/optimum/onnxruntime/quantization.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model_or_path, file_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not find any ONNX model file in {model_or_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnx_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    144\u001b[0m                     \u001b[0;34mf\"Found too many ONNX model files in {model_or_path}. {ort_quantizer_error_message}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found too many ONNX model files in /Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:/ner/model. ORTQuantizer does not support multi-file quantization. Please create separate ORTQuantizer instances for each model/file, by passing the argument `file_name` to ORTQuantizer.from_pretrained()."
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(model_id)\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "model_quantized_path = quantizer.quantize(\n",
    "    save_dir=model_id + \"/model-quantized.onnx\",\n",
    "    quantization_config=qconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model=ORTModelForTokenClassification.from_pretrained(\"/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:/ner/model/model-quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dir = \"/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:/ner/model/model-quantized\"\n",
    "os.path.isdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_ner = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'FIRSTNAME', 'score': 0.80968964, 'word': ' John.', 'start': 10, 'end': 16}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:393: UserWarning: Tokenizer does not support real words, using fallback heuristic\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text = \"My name is John. \"\n",
    "pred = onnx_ner(text)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'CITY', 'score': 0.8359974, 'word': ' London.', 'start': 21, 'end': 29}]\n",
      "{'text': 'Taylor Swift lives in London.', 'ents': [{'start': 21, 'end': 29, 'label': 'CITY'}], 'title': None}\n",
      "{'text': 'Taylor Swift lives in London.', 'ents': [{'start': 21, 'end': 29, 'label': 'CITY'}], 'title': None}\n"
     ]
    }
   ],
   "source": [
    "class NEROnnxModel():\n",
    "    def __call__(self, text: str) -> str:\n",
    "        # load quantized model and tokenizers\n",
    "        # quantized_model = ORTModelForTokenClassification.from_pretrained(\"/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/model-quantized\")\n",
    "        # tokenizerdisq = AutoTokenizer.from_pretrained(\"/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/model-quantized\")\n",
    "\n",
    "        model_id='/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:/ner/model/model-quantized'\n",
    "        tokenizer_id='/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:'\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "        model = ORTModelForTokenClassification.from_pretrained(model_id)\n",
    "        tokenizer.model_input_names = ['input_ids', 'attention_mask']       \n",
    "        #create transformers pipeline\n",
    "        # onnx_ner = pipeline(\"token-classification\", model=quantized_model, tokenizer=tokenizerdisq, aggregation_strategy=\"first\")\n",
    "        onnx_ner = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")\n",
    "\n",
    "        #run the pipeline and return the results\n",
    "        pred = onnx_ner(text)\n",
    "        print(pred)\n",
    "        pred = self.sanitise_data(text, pred)\n",
    "        print(pred)\n",
    "        return pred\n",
    "    \n",
    "    def sanitise_data(self, text: str, entities: list) -> dict:\n",
    "        return_entities=[]\n",
    "        for entity in entities:\n",
    "            santitise_entity={}\n",
    "            santitise_entity['start'] = entity['start']\n",
    "            santitise_entity['end'] = entity['end']\n",
    "            santitise_entity['label'] = entity['entity_group']\n",
    "            return_entities.append(santitise_entity)\n",
    "        return {'text':text, 'ents':return_entities, 'title': None}\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    text = \"Taylor Swift lives in London.\"\n",
    "    pipe = NEROnnxModel()\n",
    "    results = pipe(text)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<onnxruntime.capi.onnxruntime_inference_collection.InferenceSession object at 0x7f7a0574dd90>\n",
      "{'input_ids': array([[    1,  2722, 36671,  1074,    11,   188,   469,     4,     2]]), 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[[  7   7 103   7   7  59  67   7   7]]\n",
      "{'sentence': 'Jack Sparrow lives in New York.', 'input_ids': array([    1,  2722, 36671,  1074,    11,   188,   469,     4,     2]), 'predictions': array([  7,   7, 103,   7,   7,  59,  67,   7,   7]), 'offset_mapping': array([[ 0,  0],\n",
      "       [ 0,  4],\n",
      "       [ 4, 12],\n",
      "       [12, 18],\n",
      "       [18, 21],\n",
      "       [21, 25],\n",
      "       [25, 30],\n",
      "       [30, 31],\n",
      "       [ 0,  0]]), 'special_tokens_mask': array([1, 0, 0, 0, 0, 0, 0, 0, 1])}\n",
      "{'text': 'Jack Sparrow lives in New York.', 'ents': [{'start': 4, 'end': 12, 'label': 'FULLNAME'}, {'start': 21, 'end': 30, 'label': 'STATE'}], 'title': None}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from onnxruntime import InferenceSession, SessionOptions, GraphOptimizationLevel\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Tuple\n",
    "from onnxruntime.capi.onnxruntime_pybind11_state import InvalidArgument\n",
    "\n",
    "def create_model_for_provider(model_path, provider='CPUExecutionProvider'):\n",
    "    '''\n",
    "    Create CPU inference session for ONNX runtime to boost performance\n",
    "    :param model_path: path to *.onnx model\n",
    "    :param provider: CPU/CUDA\n",
    "    :return: onnx runtime session\n",
    "    '''\n",
    "    options = SessionOptions()\n",
    "    options.intra_op_num_threads = 1\n",
    "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    session = InferenceSession(str(model_path), options, providers=[provider])\n",
    "    session.disable_fallback()\n",
    "    print(session)\n",
    "    return session\n",
    "\n",
    "class NEROnnxModel():\n",
    "    \"\"\"Build NER onnx model and aggregate results into data to be rendered\"\"\"\n",
    "    def __init__(self, quant=True) -> None:\n",
    "        # if quant:\n",
    "        model_path = \"/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:/quant/model.onnx\"\n",
    "        # else: \n",
    "        #     model_path = \"/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/ner/model/model.onnx\"\n",
    "        self.model = create_model_for_provider(model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:')\n",
    "        self.id2label= {\n",
    "    \"0\": \"B-PREFIX\",\n",
    "    \"1\": \"I-PREFIX\",\n",
    "    \"2\": \"B-FIRSTNAME\",\n",
    "    \"3\": \"I-FIRSTNAME\",\n",
    "    \"4\": \"B-MIDDLENAME\",\n",
    "    \"5\": \"B-LASTNAME\",\n",
    "    \"6\": \"I-LASTNAME\",\n",
    "    \"7\": \"O\",\n",
    "    \"8\": \"B-JOBDESCRIPTOR\",\n",
    "    \"9\": \"B-JOBTITLE\",\n",
    "    \"10\": \"I-JOBTITLE\",\n",
    "    \"11\": \"B-COMPANY_NAME\",\n",
    "    \"12\": \"I-COMPANY_NAME\",\n",
    "    \"13\": \"B-JOBAREA\",\n",
    "    \"14\": \"B-EMAIL\",\n",
    "    \"15\": \"I-EMAIL\",\n",
    "    \"16\": \"B-TIME\",\n",
    "    \"17\": \"I-TIME\",\n",
    "    \"18\": \"B-DATE\",\n",
    "    \"19\": \"I-DATE\",\n",
    "    \"20\": \"B-URL\",\n",
    "    \"21\": \"I-URL\",\n",
    "    \"22\": \"B-BITCOINADDRESS\",\n",
    "    \"23\": \"I-BITCOINADDRESS\",\n",
    "    \"24\": \"B-ETHEREUMADDRESS\",\n",
    "    \"25\": \"I-ETHEREUMADDRESS\",\n",
    "    \"26\": \"B-ACCOUNTNAME\",\n",
    "    \"27\": \"I-ACCOUNTNAME\",\n",
    "    \"28\": \"B-IBAN\",\n",
    "    \"29\": \"I-IBAN\",\n",
    "    \"30\": \"B-ACCOUNTNUMBER\",\n",
    "    \"31\": \"I-ACCOUNTNUMBER\",\n",
    "    \"32\": \"B-BIC\",\n",
    "    \"33\": \"I-BIC\",\n",
    "    \"34\": \"B-IPV4\",\n",
    "    \"35\": \"I-IPV4\",\n",
    "    \"36\": \"B-STREETADDRESS\",\n",
    "    \"37\": \"I-STREETADDRESS\",\n",
    "    \"38\": \"B-CITY\",\n",
    "    \"39\": \"I-CITY\",\n",
    "    \"40\": \"B-ZIPCODE\",\n",
    "    \"41\": \"I-ZIPCODE\",\n",
    "    \"42\": \"B-USERNAME\",\n",
    "    \"43\": \"I-USERNAME\",\n",
    "    \"44\": \"B-IPV6\",\n",
    "    \"45\": \"I-IPV6\",\n",
    "    \"46\": \"B-CREDITCARDNUMBER\",\n",
    "    \"47\": \"I-CREDITCARDNUMBER\",\n",
    "    \"48\": \"B-VEHICLEVIN\",\n",
    "    \"49\": \"I-VEHICLEVIN\",\n",
    "    \"50\": \"B-SUFFIX\",\n",
    "    \"51\": \"I-SUFFIX\",\n",
    "    \"52\": \"B-AMOUNT\",\n",
    "    \"53\": \"I-AMOUNT\",\n",
    "    \"54\": \"B-CURRENCY\",\n",
    "    \"55\": \"I-CURRENCY\",\n",
    "    \"56\": \"B-PASSWORD\",\n",
    "    \"57\": \"I-PASSWORD\",\n",
    "    \"58\": \"B-JOBTYPE\",\n",
    "    \"59\": \"B-STATE\",\n",
    "    \"60\": \"B-BUILDINGNUMBER\",\n",
    "    \"61\": \"I-BUILDINGNUMBER\",\n",
    "    \"62\": \"B-VEHICLEVRM\",\n",
    "    \"63\": \"I-VEHICLEVRM\",\n",
    "    \"64\": \"B-PHONEIMEI\",\n",
    "    \"65\": \"I-PHONEIMEI\",\n",
    "    \"66\": \"I-JOBAREA\",\n",
    "    \"67\": \"I-STATE\",\n",
    "    \"68\": \"B-COUNTY\",\n",
    "    \"69\": \"B-CURRENCYNAME\",\n",
    "    \"70\": \"I-CURRENCYNAME\",\n",
    "    \"71\": \"B-CURRENCYSYMBOL\",\n",
    "    \"72\": \"B-MASKEDNUMBER\",\n",
    "    \"73\": \"I-MASKEDNUMBER\",\n",
    "    \"74\": \"B-PHONE_NUMBER\",\n",
    "    \"75\": \"I-PHONE_NUMBER\",\n",
    "    \"76\": \"B-SECONDARYADDRESS\",\n",
    "    \"77\": \"I-SECONDARYADDRESS\",\n",
    "    \"78\": \"B-SSN\",\n",
    "    \"79\": \"I-SSN\",\n",
    "    \"80\": \"B-CURRENCYCODE\",\n",
    "    \"81\": \"B-LITECOINADDRESS\",\n",
    "    \"82\": \"I-LITECOINADDRESS\",\n",
    "    \"83\": \"B-MAC\",\n",
    "    \"84\": \"I-MAC\",\n",
    "    \"85\": \"B-CREDITCARDISSUER\",\n",
    "    \"86\": \"I-CREDITCARDISSUER\",\n",
    "    \"87\": \"B-CREDITCARDCVV\",\n",
    "    \"88\": \"I-CREDITCARDCVV\",\n",
    "    \"89\": \"B-USERAGENT\",\n",
    "    \"90\": \"I-USERAGENT\",\n",
    "    \"91\": \"B-IP\",\n",
    "    \"92\": \"I-IP\",\n",
    "    \"93\": \"B-SEX\",\n",
    "    \"94\": \"B-STREET\",\n",
    "    \"95\": \"I-STREET\",\n",
    "    \"96\": \"B-PIN\",\n",
    "    \"97\": \"I-PIN\",\n",
    "    \"98\": \"I-JOBTYPE\",\n",
    "    \"99\": \"I-MIDDLENAME\",\n",
    "    \"100\": \"I-CURRENCYCODE\",\n",
    "    \"101\": \"I-CURRENCYSYMBOL\",\n",
    "    \"102\": \"B-FULLNAME\",\n",
    "    \"103\": \"I-FULLNAME\",\n",
    "    \"104\": \"B-NAME\",\n",
    "    \"105\": \"I-NAME\",\n",
    "    \"106\": \"B-GENDER\",\n",
    "    \"107\": \"B-NUMBER\",\n",
    "    \"108\": \"I-NUMBER\",\n",
    "    \"109\": \"I-GENDER\",\n",
    "    \"110\": \"B-NEARBYGPSCOORDINATE\",\n",
    "    \"111\": \"I-NEARBYGPSCOORDINATE\",\n",
    "    \"112\": \"B-DISPLAYNAME\",\n",
    "    \"113\": \"I-DISPLAYNAME\",\n",
    "    \"114\": \"B-SEXTYPE\",\n",
    "    \"115\": \"B-ORDINALDIRECTION\"\n",
    "  }\n",
    "\n",
    "    def __call__(self, sentence: str) -> str:\n",
    "        # get model inputs and other required arrays\n",
    "        model_inputs = self.tokenizer(sentence, return_tensors=\"np\",\n",
    "            return_special_tokens_mask=True, return_offsets_mapping=True)\n",
    "        special_tokens_mask = model_inputs.pop(\"special_tokens_mask\")[0]\n",
    "        offset_mapping = model_inputs.pop(\"offset_mapping\")[0]\n",
    "        logits = None\n",
    "        print(model_inputs)\n",
    "        try:\n",
    "            logits = self.model.run(None, dict(model_inputs))[0]\n",
    "        except InvalidArgument as e:\n",
    "            if 'token_type_ids' in str(e):\n",
    "                model_inputs.pop('token_type_ids', None)\n",
    "        # pass to model\n",
    "        logits = self.model.run(None, dict(model_inputs))[0]\n",
    "        print(np.argmax(logits, axis=2))\n",
    "        predictions = np.argmax(logits, axis=2)[0]\n",
    "        input_ids = model_inputs[\"input_ids\"][0]\n",
    "        model_outputs = {\"sentence\": sentence, \"input_ids\": input_ids, \"predictions\": predictions, \n",
    "            \"offset_mapping\": offset_mapping, \"special_tokens_mask\": special_tokens_mask}\n",
    "        print(model_outputs)\n",
    "        # aggregate entitity information\n",
    "        pre_entities = self.gather_pre_entities(**model_outputs)\n",
    "        entities = self.aggregate_words(pre_entities)\n",
    "        entities = self.group_entities(entities)\n",
    "        results = self.build_final_output(sentence, entities)\n",
    "        return results\n",
    "\n",
    "    def gather_pre_entities(self, sentence: str, input_ids: np.ndarray, predictions: List[int],\n",
    "            offset_mapping: List[Tuple[int, int]], special_tokens_mask: np.ndarray) -> List[dict]:\n",
    "        \"\"\"Fuse various numpy arrays into dicts with all the information needed for aggregation\"\"\"\n",
    "        pre_entities = []\n",
    "        for idx, pred in enumerate(predictions):\n",
    "            # Filter special_tokens, they should only occur at the sentence boundaries \n",
    "            if special_tokens_mask[idx]:\n",
    "                continue\n",
    "            word = self.tokenizer.convert_ids_to_tokens(int(input_ids[idx]))\n",
    "            start_ind, end_ind = offset_mapping[idx]\n",
    "            word_ref = sentence[start_ind:end_ind]\n",
    "            is_subword = len(word) != len(word_ref)\n",
    "            if int(input_ids[idx]) == self.tokenizer.unk_token_id:\n",
    "                word = word_ref\n",
    "                is_subword = False\n",
    "            pre_entity = {\"word\": word, \"entity\": self.id2label[str(pred)], \"start\": start_ind,\n",
    "                \"end\": end_ind, \"index\": idx, \"is_subword\": is_subword}\n",
    "            pre_entities.append(pre_entity)\n",
    "        return pre_entities\n",
    "\n",
    "    def aggregate_word(self, entities: List[dict]) -> dict:\n",
    "        \"\"\"Aggregate sub-words together and make sure entities align\"\"\"\n",
    "        word = self.tokenizer.convert_tokens_to_string([entity[\"word\"] for entity in entities])\n",
    "        entity = entities[0][\"entity\"]\n",
    "        new_entity = {\"word\": word, \"entity\": entity, \"start\": entities[0][\"start\"],\n",
    "            \"end\": entities[-1][\"end\"]}\n",
    "        return new_entity\n",
    "\n",
    "    def aggregate_words(self, entities: List[dict]) -> List[dict]:\n",
    "        \"\"\"Override tokens from a given word that disagree to force agreement on word boundaries\"\"\"\n",
    "        word_entities = []\n",
    "        word_group = None\n",
    "        for entity in entities:\n",
    "            if word_group is None:\n",
    "                word_group = [entity]\n",
    "            elif entity[\"is_subword\"]:\n",
    "                word_group.append(entity)\n",
    "            else:\n",
    "                word_entities.append(self.aggregate_word(word_group))\n",
    "                word_group = [entity]\n",
    "        # add last item\n",
    "        word_entities.append(self.aggregate_word(word_group))\n",
    "        return word_entities\n",
    "\n",
    "    def group_sub_entities(self, entities: List[dict]) -> dict:\n",
    "        \"\"\"Group together the adjacent tokens with the same entity predicted\"\"\"\n",
    "        # Get the first entity in the entity group\n",
    "        entity = entities[0][\"entity\"].split(\"-\")[-1]\n",
    "        tokens = [entity[\"word\"] for entity in entities]\n",
    "        entity_group = {\"entity_group\": entity, \"word\": self.tokenizer.convert_tokens_to_string(tokens),\n",
    "            \"start\": entities[0][\"start\"], \"end\": entities[-1][\"end\"]}\n",
    "        return entity_group\n",
    "\n",
    "    def get_tag(self, entity_name: str) -> Tuple[str, str]:\n",
    "        if entity_name.startswith(\"B-\"):\n",
    "            bi = \"B\"\n",
    "            tag = entity_name[2:]\n",
    "        elif entity_name.startswith(\"I-\"):\n",
    "            bi = \"I\"\n",
    "            tag = entity_name[2:]\n",
    "        else:\n",
    "            # if not in B-, I- format default to I- for continuation\n",
    "            bi = \"I\"\n",
    "            tag = entity_name\n",
    "        return bi, tag\n",
    "\n",
    "    def group_entities(self, entities: List[dict]) -> List[dict]:\n",
    "        \"\"\"Find and group together the adjacent tokens with the same entity predicted\"\"\"\n",
    "        entity_groups = []\n",
    "        entity_group_disagg = []\n",
    "        for entity in entities:\n",
    "            if not entity_group_disagg:\n",
    "                entity_group_disagg.append(entity)\n",
    "                continue\n",
    "            # if the current entity is similar and adjacent to the previous entity, \n",
    "            # append it to the disaggregated entity group\n",
    "            bi, tag = self.get_tag(entity[\"entity\"])\n",
    "            last_bi, last_tag = self.get_tag(entity_group_disagg[-1][\"entity\"])\n",
    "            if tag == last_tag and bi != \"B\":\n",
    "                # modify subword type to be previous_type\n",
    "                entity_group_disagg.append(entity)\n",
    "            else:\n",
    "                # if the current entity is different from the previous entity\n",
    "                # aggregate the disaggregated entity group\n",
    "                entity_groups.append(self.group_sub_entities(entity_group_disagg))\n",
    "                entity_group_disagg = [entity]\n",
    "        if entity_group_disagg:\n",
    "            # it's the last entity, add it to the entity groups\n",
    "            entity_groups.append(self.group_sub_entities(entity_group_disagg))\n",
    "        return entity_groups\n",
    "\n",
    "    def build_final_output(self, sentence: str, entity_groups: List[dict]) -> List[dict]:\n",
    "        entities = []\n",
    "        for entity_group in entity_groups:\n",
    "            if entity_group['entity_group'] == 'O':\n",
    "                continue\n",
    "            else:\n",
    "                entry = {}\n",
    "                entry['start'] = entity_group['start']\n",
    "                entry['end'] = entity_group['end']\n",
    "                entry['label'] = entity_group['entity_group']\n",
    "                entities.append(entry)\n",
    "        render_data = {'text': sentence, 'ents': entities, 'title': None}\n",
    "        return render_data\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # model_checkpoint = \"ner/model/model.onnx\"\n",
    "    sentence = \"Jack Sparrow lives in New York.\"\n",
    "    # sentence = \"Albert Einstein was born at Ulm, in Württemberg, Germany, on March 14, 1879. Six weeks later the family moved to Munich, where he later on began his schooling at the Luitpold Gymnasium. Later, they moved to Italy and Albert continued his education at Aarau, Switzerland and in 1896 he entered the Swiss Federal Polytechnic School in Zurich to be trained as a teacher in physics and mathematics.\"\n",
    "    pipe = NEROnnxModel()\n",
    "    results = pipe(sentence)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([[    1,  2722, 36671,  1074,    11,   730,     4,     2]]), 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[[7 7 7 7 7 7 7 7]]\n",
      "{'sentence': 'Jack Sparrow lives in America.', 'input_ids': array([    1,  2722, 36671,  1074,    11,   730,     4,     2]), 'predictions': array([7, 7, 7, 7, 7, 7, 7, 7]), 'offset_mapping': array([[ 0,  0],\n",
      "       [ 0,  4],\n",
      "       [ 4, 12],\n",
      "       [12, 18],\n",
      "       [18, 21],\n",
      "       [21, 29],\n",
      "       [29, 30],\n",
      "       [ 0,  0]]), 'special_tokens_mask': array([1, 0, 0, 0, 0, 0, 0, 1])}\n",
      "{'text': 'Jack Sparrow lives in America.', 'ents': [], 'title': None}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from onnxruntime import InferenceSession, SessionOptions, GraphOptimizationLevel\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Tuple\n",
    "from onnxruntime.capi.onnxruntime_pybind11_state import InvalidArgument\n",
    "\n",
    "def create_model_for_provider(model_path, provider='CPUExecutionProvider'):\n",
    "    '''\n",
    "    Create CPU inference session for ONNX runtime to boost performance\n",
    "    :param model_path: path to *.onnx model\n",
    "    :param provider: CPU/CUDA\n",
    "    :return: onnx runtime session\n",
    "    '''\n",
    "    options = SessionOptions()\n",
    "    options.intra_op_num_threads = 1\n",
    "    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "    session = InferenceSession(str(model_path), options, providers=[provider])\n",
    "    session.disable_fallback()\n",
    "    return session\n",
    "\n",
    "class NEROnnxModel():\n",
    "    \"\"\"Build NER onnx model and aggregate results into data to be rendered\"\"\"\n",
    "    def __init__(self, quant=True) -> None:\n",
    "        # if quant:\n",
    "        model_path = \"/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:/quant/model-quant1.onnx\"\n",
    "        # else: \n",
    "        #     model_path = \"/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/ner/model/model.onnx\"\n",
    "        self.model = create_model_for_provider(model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('/Users/salonisalian/Desktop/University Files/Year 3/third_year_project/pii_onnx:')\n",
    "        self.id2label= {\n",
    "    \"0\": \"B-PREFIX\",\n",
    "    \"1\": \"I-PREFIX\",\n",
    "    \"2\": \"B-FIRSTNAME\",\n",
    "    \"3\": \"I-FIRSTNAME\",\n",
    "    \"4\": \"B-MIDDLENAME\",\n",
    "    \"5\": \"B-LASTNAME\",\n",
    "    \"6\": \"I-LASTNAME\",\n",
    "    \"7\": \"O\",\n",
    "    \"8\": \"B-JOBDESCRIPTOR\",\n",
    "    \"9\": \"B-JOBTITLE\",\n",
    "    \"10\": \"I-JOBTITLE\",\n",
    "    \"11\": \"B-COMPANY_NAME\",\n",
    "    \"12\": \"I-COMPANY_NAME\",\n",
    "    \"13\": \"B-JOBAREA\",\n",
    "    \"14\": \"B-EMAIL\",\n",
    "    \"15\": \"I-EMAIL\",\n",
    "    \"16\": \"B-TIME\",\n",
    "    \"17\": \"I-TIME\",\n",
    "    \"18\": \"B-DATE\",\n",
    "    \"19\": \"I-DATE\",\n",
    "    \"20\": \"B-URL\",\n",
    "    \"21\": \"I-URL\",\n",
    "    \"22\": \"B-BITCOINADDRESS\",\n",
    "    \"23\": \"I-BITCOINADDRESS\",\n",
    "    \"24\": \"B-ETHEREUMADDRESS\",\n",
    "    \"25\": \"I-ETHEREUMADDRESS\",\n",
    "    \"26\": \"B-ACCOUNTNAME\",\n",
    "    \"27\": \"I-ACCOUNTNAME\",\n",
    "    \"28\": \"B-IBAN\",\n",
    "    \"29\": \"I-IBAN\",\n",
    "    \"30\": \"B-ACCOUNTNUMBER\",\n",
    "    \"31\": \"I-ACCOUNTNUMBER\",\n",
    "    \"32\": \"B-BIC\",\n",
    "    \"33\": \"I-BIC\",\n",
    "    \"34\": \"B-IPV4\",\n",
    "    \"35\": \"I-IPV4\",\n",
    "    \"36\": \"B-STREETADDRESS\",\n",
    "    \"37\": \"I-STREETADDRESS\",\n",
    "    \"38\": \"B-CITY\",\n",
    "    \"39\": \"I-CITY\",\n",
    "    \"40\": \"B-ZIPCODE\",\n",
    "    \"41\": \"I-ZIPCODE\",\n",
    "    \"42\": \"B-USERNAME\",\n",
    "    \"43\": \"I-USERNAME\",\n",
    "    \"44\": \"B-IPV6\",\n",
    "    \"45\": \"I-IPV6\",\n",
    "    \"46\": \"B-CREDITCARDNUMBER\",\n",
    "    \"47\": \"I-CREDITCARDNUMBER\",\n",
    "    \"48\": \"B-VEHICLEVIN\",\n",
    "    \"49\": \"I-VEHICLEVIN\",\n",
    "    \"50\": \"B-SUFFIX\",\n",
    "    \"51\": \"I-SUFFIX\",\n",
    "    \"52\": \"B-AMOUNT\",\n",
    "    \"53\": \"I-AMOUNT\",\n",
    "    \"54\": \"B-CURRENCY\",\n",
    "    \"55\": \"I-CURRENCY\",\n",
    "    \"56\": \"B-PASSWORD\",\n",
    "    \"57\": \"I-PASSWORD\",\n",
    "    \"58\": \"B-JOBTYPE\",\n",
    "    \"59\": \"B-STATE\",\n",
    "    \"60\": \"B-BUILDINGNUMBER\",\n",
    "    \"61\": \"I-BUILDINGNUMBER\",\n",
    "    \"62\": \"B-VEHICLEVRM\",\n",
    "    \"63\": \"I-VEHICLEVRM\",\n",
    "    \"64\": \"B-PHONEIMEI\",\n",
    "    \"65\": \"I-PHONEIMEI\",\n",
    "    \"66\": \"I-JOBAREA\",\n",
    "    \"67\": \"I-STATE\",\n",
    "    \"68\": \"B-COUNTY\",\n",
    "    \"69\": \"B-CURRENCYNAME\",\n",
    "    \"70\": \"I-CURRENCYNAME\",\n",
    "    \"71\": \"B-CURRENCYSYMBOL\",\n",
    "    \"72\": \"B-MASKEDNUMBER\",\n",
    "    \"73\": \"I-MASKEDNUMBER\",\n",
    "    \"74\": \"B-PHONE_NUMBER\",\n",
    "    \"75\": \"I-PHONE_NUMBER\",\n",
    "    \"76\": \"B-SECONDARYADDRESS\",\n",
    "    \"77\": \"I-SECONDARYADDRESS\",\n",
    "    \"78\": \"B-SSN\",\n",
    "    \"79\": \"I-SSN\",\n",
    "    \"80\": \"B-CURRENCYCODE\",\n",
    "    \"81\": \"B-LITECOINADDRESS\",\n",
    "    \"82\": \"I-LITECOINADDRESS\",\n",
    "    \"83\": \"B-MAC\",\n",
    "    \"84\": \"I-MAC\",\n",
    "    \"85\": \"B-CREDITCARDISSUER\",\n",
    "    \"86\": \"I-CREDITCARDISSUER\",\n",
    "    \"87\": \"B-CREDITCARDCVV\",\n",
    "    \"88\": \"I-CREDITCARDCVV\",\n",
    "    \"89\": \"B-USERAGENT\",\n",
    "    \"90\": \"I-USERAGENT\",\n",
    "    \"91\": \"B-IP\",\n",
    "    \"92\": \"I-IP\",\n",
    "    \"93\": \"B-SEX\",\n",
    "    \"94\": \"B-STREET\",\n",
    "    \"95\": \"I-STREET\",\n",
    "    \"96\": \"B-PIN\",\n",
    "    \"97\": \"I-PIN\",\n",
    "    \"98\": \"I-JOBTYPE\",\n",
    "    \"99\": \"I-MIDDLENAME\",\n",
    "    \"100\": \"I-CURRENCYCODE\",\n",
    "    \"101\": \"I-CURRENCYSYMBOL\",\n",
    "    \"102\": \"B-FULLNAME\",\n",
    "    \"103\": \"I-FULLNAME\",\n",
    "    \"104\": \"B-NAME\",\n",
    "    \"105\": \"I-NAME\",\n",
    "    \"106\": \"B-GENDER\",\n",
    "    \"107\": \"B-NUMBER\",\n",
    "    \"108\": \"I-NUMBER\",\n",
    "    \"109\": \"I-GENDER\",\n",
    "    \"110\": \"B-NEARBYGPSCOORDINATE\",\n",
    "    \"111\": \"I-NEARBYGPSCOORDINATE\",\n",
    "    \"112\": \"B-DISPLAYNAME\",\n",
    "    \"113\": \"I-DISPLAYNAME\",\n",
    "    \"114\": \"B-SEXTYPE\",\n",
    "    \"115\": \"B-ORDINALDIRECTION\"\n",
    "  }\n",
    "\n",
    "    def __call__(self, sentence: str) -> str:\n",
    "        # get model inputs and other required arrays\n",
    "        model_inputs = self.tokenizer(sentence, return_tensors=\"np\",\n",
    "            return_special_tokens_mask=True, return_offsets_mapping=True)\n",
    "        special_tokens_mask = model_inputs.pop(\"special_tokens_mask\")[0]\n",
    "        offset_mapping = model_inputs.pop(\"offset_mapping\")[0]\n",
    "        logits = None\n",
    "        print(model_inputs)\n",
    "        try:\n",
    "            logits = self.model.run(None, dict(model_inputs))[0]\n",
    "        except InvalidArgument as e:\n",
    "            if 'token_type_ids' in str(e):\n",
    "                model_inputs.pop('token_type_ids', None)\n",
    "        # pass to model\n",
    "        logits = self.model.run(None, dict(model_inputs))[0]\n",
    "        print(np.argmax(logits, axis=2))\n",
    "        predictions = np.argmax(logits, axis=2)[0]\n",
    "        input_ids = model_inputs[\"input_ids\"][0]\n",
    "        model_outputs = {\"sentence\": sentence, \"input_ids\": input_ids, \"predictions\": predictions, \n",
    "            \"offset_mapping\": offset_mapping, \"special_tokens_mask\": special_tokens_mask}\n",
    "        print(model_outputs)\n",
    "        # aggregate entitity information\n",
    "        pre_entities = self.gather_pre_entities(**model_outputs)\n",
    "        entities = self.aggregate_words(pre_entities)\n",
    "        entities = self.group_entities(entities)\n",
    "        results = self.build_final_output(sentence, entities)\n",
    "        return results\n",
    "\n",
    "    def gather_pre_entities(self, sentence: str, input_ids: np.ndarray, predictions: List[int],\n",
    "            offset_mapping: List[Tuple[int, int]], special_tokens_mask: np.ndarray) -> List[dict]:\n",
    "        \"\"\"Fuse various numpy arrays into dicts with all the information needed for aggregation\"\"\"\n",
    "        pre_entities = []\n",
    "        for idx, pred in enumerate(predictions):\n",
    "            # Filter special_tokens, they should only occur at the sentence boundaries \n",
    "            if special_tokens_mask[idx]:\n",
    "                continue\n",
    "            word = self.tokenizer.convert_ids_to_tokens(int(input_ids[idx]))\n",
    "            start_ind, end_ind = offset_mapping[idx]\n",
    "            word_ref = sentence[start_ind:end_ind]\n",
    "            is_subword = len(word) != len(word_ref)\n",
    "            if int(input_ids[idx]) == self.tokenizer.unk_token_id:\n",
    "                word = word_ref\n",
    "                is_subword = False\n",
    "            pre_entity = {\"word\": word, \"entity\": self.id2label[str(pred)], \"start\": start_ind,\n",
    "                \"end\": end_ind, \"index\": idx, \"is_subword\": is_subword}\n",
    "            pre_entities.append(pre_entity)\n",
    "        return pre_entities\n",
    "\n",
    "    def aggregate_word(self, entities: List[dict]) -> dict:\n",
    "        \"\"\"Aggregate sub-words together and make sure entities align\"\"\"\n",
    "        word = self.tokenizer.convert_tokens_to_string([entity[\"word\"] for entity in entities])\n",
    "        entity = entities[0][\"entity\"]\n",
    "        new_entity = {\"word\": word, \"entity\": entity, \"start\": entities[0][\"start\"],\n",
    "            \"end\": entities[-1][\"end\"]}\n",
    "        return new_entity\n",
    "\n",
    "    def aggregate_words(self, entities: List[dict]) -> List[dict]:\n",
    "        \"\"\"Override tokens from a given word that disagree to force agreement on word boundaries\"\"\"\n",
    "        word_entities = []\n",
    "        word_group = None\n",
    "        for entity in entities:\n",
    "            if word_group is None:\n",
    "                word_group = [entity]\n",
    "            elif entity[\"is_subword\"]:\n",
    "                word_group.append(entity)\n",
    "            else:\n",
    "                word_entities.append(self.aggregate_word(word_group))\n",
    "                word_group = [entity]\n",
    "        # add last item\n",
    "        word_entities.append(self.aggregate_word(word_group))\n",
    "        return word_entities\n",
    "\n",
    "    def group_sub_entities(self, entities: List[dict]) -> dict:\n",
    "        \"\"\"Group together the adjacent tokens with the same entity predicted\"\"\"\n",
    "        # Get the first entity in the entity group\n",
    "        entity = entities[0][\"entity\"].split(\"-\")[-1]\n",
    "        tokens = [entity[\"word\"] for entity in entities]\n",
    "        entity_group = {\"entity_group\": entity, \"word\": self.tokenizer.convert_tokens_to_string(tokens),\n",
    "            \"start\": entities[0][\"start\"], \"end\": entities[-1][\"end\"]}\n",
    "        return entity_group\n",
    "\n",
    "    def get_tag(self, entity_name: str) -> Tuple[str, str]:\n",
    "        if entity_name.startswith(\"B-\"):\n",
    "            bi = \"B\"\n",
    "            tag = entity_name[2:]\n",
    "        elif entity_name.startswith(\"I-\"):\n",
    "            bi = \"I\"\n",
    "            tag = entity_name[2:]\n",
    "        else:\n",
    "            # if not in B-, I- format default to I- for continuation\n",
    "            bi = \"I\"\n",
    "            tag = entity_name\n",
    "        return bi, tag\n",
    "\n",
    "    def group_entities(self, entities: List[dict]) -> List[dict]:\n",
    "        \"\"\"Find and group together the adjacent tokens with the same entity predicted\"\"\"\n",
    "        entity_groups = []\n",
    "        entity_group_disagg = []\n",
    "        for entity in entities:\n",
    "            if not entity_group_disagg:\n",
    "                entity_group_disagg.append(entity)\n",
    "                continue\n",
    "            # if the current entity is similar and adjacent to the previous entity, \n",
    "            # append it to the disaggregated entity group\n",
    "            bi, tag = self.get_tag(entity[\"entity\"])\n",
    "            last_bi, last_tag = self.get_tag(entity_group_disagg[-1][\"entity\"])\n",
    "            if tag == last_tag and bi != \"B\":\n",
    "                # modify subword type to be previous_type\n",
    "                entity_group_disagg.append(entity)\n",
    "            else:\n",
    "                # if the current entity is different from the previous entity\n",
    "                # aggregate the disaggregated entity group\n",
    "                entity_groups.append(self.group_sub_entities(entity_group_disagg))\n",
    "                entity_group_disagg = [entity]\n",
    "        if entity_group_disagg:\n",
    "            # it's the last entity, add it to the entity groups\n",
    "            entity_groups.append(self.group_sub_entities(entity_group_disagg))\n",
    "        return entity_groups\n",
    "\n",
    "    def build_final_output(self, sentence: str, entity_groups: List[dict]) -> List[dict]:\n",
    "        entities = []\n",
    "        for entity_group in entity_groups:\n",
    "            if entity_group['entity_group'] == 'O':\n",
    "                continue\n",
    "            else:\n",
    "                entry = {}\n",
    "                entry['start'] = entity_group['start']\n",
    "                entry['end'] = entity_group['end']\n",
    "                entry['label'] = entity_group['entity_group']\n",
    "                entities.append(entry)\n",
    "        render_data = {'text': sentence, 'ents': entities, 'title': None}\n",
    "        return render_data\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # model_checkpoint = \"ner/model/model.onnx\"\n",
    "    sentence = \"Jack Sparrow lives in America.\"\n",
    "    # sentence = \"Albert Einstein was born at Ulm, in Württemberg, Germany, on March 14, 1879. Six weeks later the family moved to Munich, where he later on began his schooling at the Luitpold Gymnasium. Later, they moved to Italy and Albert continued his education at Aarau, Switzerland and in 1896 he entered the Swiss Federal Polytechnic School in Zurich to be trained as a teacher in physics and mathematics.\"\n",
    "    pipe = NEROnnxModel()\n",
    "    results = pipe(sentence)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[MatMul_103]\n",
      "Ignore MatMul due to non constant B: /[MatMul_134]\n",
      "Ignore MatMul due to non constant B: /[MatMul_208]\n",
      "Ignore MatMul due to non constant B: /[MatMul_256]\n",
      "Ignore MatMul due to non constant B: /[MatMul_356]\n",
      "Ignore MatMul due to non constant B: /[MatMul_387]\n",
      "Ignore MatMul due to non constant B: /[MatMul_461]\n",
      "Ignore MatMul due to non constant B: /[MatMul_509]\n",
      "Ignore MatMul due to non constant B: /[MatMul_609]\n",
      "Ignore MatMul due to non constant B: /[MatMul_640]\n",
      "Ignore MatMul due to non constant B: /[MatMul_714]\n",
      "Ignore MatMul due to non constant B: /[MatMul_762]\n",
      "Ignore MatMul due to non constant B: /[MatMul_862]\n",
      "Ignore MatMul due to non constant B: /[MatMul_893]\n",
      "Ignore MatMul due to non constant B: /[MatMul_967]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1015]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1115]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1146]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1220]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1268]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1368]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1399]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1473]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1521]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1621]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1652]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1726]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1774]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1874]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1905]\n",
      "Ignore MatMul due to non constant B: /[MatMul_1979]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2027]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2127]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2158]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2232]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2280]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2380]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2411]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2485]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2533]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2633]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2664]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2738]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2786]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2886]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2917]\n",
      "Ignore MatMul due to non constant B: /[MatMul_2991]\n",
      "Ignore MatMul due to non constant B: /[MatMul_3039]\n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "model_input = \"quant/model.onnx\"\n",
    "model_output = \"quant/model-quant1.onnx\"\n",
    "quantize_dynamic(model_input, model_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
