{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the dependencies\n",
    "pip install --upgrade-strategy eager install optimum[onnxruntime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies to run the model\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForTokenClassification\n",
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create onnx model\n",
    "tokenizerdis = AutoTokenizer.from_pretrained(\"elastic/distilbert-base-uncased-finetuned-conll03-english\")\n",
    "modeldis = ORTModelForTokenClassification.from_pretrained(\"elastic/distilbert-base-uncased-finetuned-conll03-english\", export=True)\n",
    "\n",
    "modeldis.save_pretrained('/save_directory')\n",
    "tokenizerdis.save_pretrained('/save_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(modeldis)\n",
    "quantizer_config = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "\n",
    "# apply the quantization configuration to the model and specify the directory to save the model\n",
    "model_quantized_path = quantizer.quantize(\n",
    "    save_dir=\"/model-quantized\",\n",
    "    quantization_config=quantizer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitise_data(text: str, entities: list) -> dict:\n",
    "    return_entities=[]\n",
    "    for entity in entities:\n",
    "        santitise_entity={}\n",
    "        santitise_entity['start'] = entity['start']\n",
    "        santitise_entity['end'] = entity['end']\n",
    "        santitise_entity['label'] = entity['entity_group']\n",
    "        return_entities.append(santitise_entity)\n",
    "    return {'text':text, 'ents':return_entities, 'title': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.darcypattison.com/writing/revision/400-words-exactly/#:~:text=As%20a%20self%2Dtaught%20writer,LeGuin%2C%20the%20respected%20science%20fiction\n",
    "text='As a self-taught writer who has taken the long, winding road towards writing and literary efforts, I was slow to learn about writing long sentences,\\\n",
    " both how to do so and why one might want to do so, but finally was enlightened by three articulate authors and their books: Ursula LeGuin, the respected \\\n",
    " science fiction and fantasy writer, author of The Left Hand of Darkness, and the popular children’s series, Wizard of the Earthsea, encourages long \\\n",
    " sentences in her how-to book on writing, Steering the Craft, by quoting a 354-word sentence from Mark Twain’s Huckleberry Finn, which takes the long, \\\n",
    " slow route – LeGuin calls it the “marvelously supple connections of complex syntax” – to describing the details of a sunrise over the Mississippi River\\\n",
    " , including the sights, sounds, and smells of the unfolding morning; the second book which is less artistic, but perhaps more helpful to me personally \\\n",
    " was Ann Longknife, Ph.D and K.D. Sullivan’s book, The Art of Styling Sentences: 20 Patterns for Success – look for the second edition published in 2002 \\\n",
    " – which I studied with a writing friend, KN, and found to be extremely helpful in reviewing colons, semi-colons, appostives, etc, especially as KN and \\\n",
    " I posted to mutual mailing lists and encourage each other to use the patterns correctly and creatively and learned that control of language was essential\\\n",
    "  to make the words mean what you want it to mean ( in fact, I found this book to be so useful, that I required it as a text when I taught Freshman \\\n",
    "  Composition); and third, was Dona Hickey’s wondeful book, Developing a Written Voice, a virtual gem of a book – it’s not for the faint-hearted, because\\\n",
    "   it reads like a college text book, but it’s a gem, nonetheless – which encourages the exploration of both long and short sentences, including sentence\\\n",
    "    fragments, while Hickey also gives the writer a range of options for creating coherence and cohesion among the various parts of the sentence, \\\n",
    "    including traditional rhetorical strategies such as schemes (unusual patterns of words) : schemes of balance, such as parallel structures, \\\n",
    "    antithesis and the isocolon; schemes of unusual or inverted word order, such as anastrophe and parenthesis; schemes of omission, such as ellipsis, \\\n",
    "    asyndeton or polysyndeton; schemes of repetition, such as alliteration, polyptoton, assonance, anaphora, epistrophe, epanalepsis, anadiplosis,\\\n",
    "     tricolon, chiasmus, and of course, long lists – all useful tools to create long sentences and keep them understandable. Writing long is fun. \\\n",
    "     Really. Try it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerdis = AutoTokenizer.from_pretrained(\"/save_directory\")\n",
    "modeldis = ORTModelForTokenClassification.from_pretrained(\"/model-quantized\")\n",
    "\n",
    "#create transformers pipeline\n",
    "onnx_ner = pipeline(\"token-classification\", model=modeldis, tokenizer=tokenizerdis, aggregation_strategy='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the time the model takes to run and return results\n",
    "import time\n",
    "for i in range(5):\n",
    "    start_time=time.time()\n",
    "    pred = onnx_ner(text)\n",
    "    pred = sanitise_data(text, pred)\n",
    "    end_time=time.time()\n",
    "    print (f\"The time to return results for 400 words of text is: {(end_time-start_time):.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
